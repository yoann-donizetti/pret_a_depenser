---
title: Pret a DeÌpenser API
emoji: ðŸ¦
colorFrom: blue
colorTo: purple
sdk: docker
app_port: 7860
pinned: false
---



# PrÃªt Ã  DÃ©penser â€” Credit Scoring (MLOps)

Ce projet consiste Ã  dÃ©velopper un modÃ¨le de **credit scoring** permettant dâ€™estimer la probabilitÃ© de dÃ©faut dâ€™un client.

Il sâ€™inscrit dans une dÃ©marche **MLOps complÃ¨te**, allant du tracking dâ€™expÃ©riences (**MLflow**) jusquâ€™au dÃ©ploiement dâ€™une **API FastAPI**, prÃªte Ã  Ãªtre conteneurisÃ©e et dÃ©ployÃ©e (Hugging Face / Docker).

Le projet est organisÃ© en deux parties :

- **Partie 1/2 : ModÃ©lisation + tracking MLflow**
- **Partie 2/2 : DÃ©ploiement API + packaging + monitoring**

---
## Sommaire

- [PrÃªt Ã  DÃ©penser â€” Credit Scoring (MLOps)](#prÃªt-Ã -dÃ©penser--credit-scoring-mlops)
- [Objectif du projet](#objectif-du-projet)
- [Enjeux mÃ©tier](#enjeux-mÃ©tier)
- [Partie 1/2 â€” ModÃ©lisation & Tracking MLflow](#partie-12--modÃ©lisation--tracking-mlflow)
  - [Travaux rÃ©alisÃ©s](#travaux-rÃ©alisÃ©s)
  - [ModÃ¨le final retenu](#modÃ¨le-final-retenu)
- [Partie 2/2 â€” DÃ©ploiement API (FastAPI)](#partie-22--dÃ©ploiement-api-fastapi)
- [ReproductibilitÃ©](#reproductibilitÃ©)
- [Structure du projet](#structure-du-projet)
- [Installation rapide](#installation-rapide)
- [Lancer MLflow UI (optionnel)](#lancer-mlflow-ui-optionnel)
- [Lancer lâ€™API en local](#lancer-lapi-en-local)
- [Exemple d'appel API](#exemple-dappel-api)
  - [Healthcheck](#healthcheck)
  - [PrÃ©diction](#prÃ©diction)
- [Chargement du modÃ¨le (local ou Hugging Face)](#chargement-du-modÃ¨le-local-ou-hugging-face)
  - [Mode local](#mode-local)
  - [Mode Hugging Face](#mode-hugging-face)
- [Tests unitaires](#tests-unitaires)
- [Run (Production / Hugging Face Space)](#run-production--hugging-face-space)
  - [Lancer en local](#lancer-en-local)
  - [Lancer via Docker (exemple)](#lancer-via-docker-exemple)
  - [DÃ©ploiement automatique](#dÃ©ploiement-automatique)
- [Secrets / SÃ©curitÃ©](#secrets--sÃ©curitÃ©)
  - [Variables dâ€™environnement](#variables-denvironnement)
  - [Gestion des tokens](#gestion-des-tokens)
  - [Validation des entrÃ©es API](#validation-des-entrÃ©es-api)
- [RÃ©sultat final](#rÃ©sultat-final)

---

## Objectif du projet

Lâ€™entreprise *PrÃªt Ã  DÃ©penser* propose des crÃ©dits Ã  la consommation Ã  des clients ayant peu ou pas dâ€™historique bancaire.

Lâ€™objectif est de construire un modÃ¨le capable de :

- prÃ©dire la **probabilitÃ© de dÃ©faut**
- produire une dÃ©cision automatique (**ACCEPTED / REFUSED**)
- respecter une contrainte mÃ©tier : **FN >> FP**

---

## Enjeux mÃ©tier

Le dataset est fortement dÃ©sÃ©quilibrÃ© et les erreurs nâ€™ont pas le mÃªme impact :

- **FN (mauvais client prÃ©dit bon)** = perte financiÃ¨re importante
- **FP (bon client prÃ©dit mauvais)** = manque Ã  gagner

Mise en place dâ€™une fonction de coÃ»t mÃ©tier :

**CoÃ»t = 10 Ã— FN + 1 Ã— FP**

---

## Partie 1/2 â€” ModÃ©lisation & Tracking MLflow

### Travaux rÃ©alisÃ©s

- Fusion et agrÃ©gation de tables multi-sources
- Dataset initial : **1658 variables**
- Benchmark de plusieurs modÃ¨les :
  - Logistic Regression
  - Random Forest
  - LightGBM
  - XGBoost
  - CatBoost
  - MLP
- SÃ©lection progressive des variables :
  - rÃ©duction de **1658 â†’ 125 features**
- Tuning dâ€™hyperparamÃ¨tres
- Optimisation du seuil mÃ©tier
- InterprÃ©tabilitÃ© avec SHAP :
  - explication globale
  - explication locale

### ModÃ¨le final retenu

Le modÃ¨le **CatBoostClassifier** est retenu car il obtient le **meilleur coÃ»t mÃ©tier** sur le jeu de test tout en conservant des performances stables.

---

## Partie 2/2 â€” DÃ©ploiement API (FastAPI)

Lâ€™API permet de prÃ©dire le risque de dÃ©faut Ã  partir dâ€™un JSON reprÃ©sentant un client.

Endpoints :

- `GET /health` â†’ Ã©tat de disponibilitÃ©
- `POST /predict` â†’ prÃ©diction score + dÃ©cision

---

## ReproductibilitÃ©

Le projet est reproductible en exÃ©cutant les notebooks dans lâ€™ordre :

- `notebooks/01_data_preparation/` : prÃ©paration et agrÃ©gation des tables
- `notebooks/02_benchmark/` : benchmark des modÃ¨les
- `notebooks/03_modeling/` : sÃ©lection features, tuning, seuil mÃ©tier, modÃ¨le final
- `notebooks/04_preparation_API/` : gÃ©nÃ©ration du modÃ¨le et artifacts pour lâ€™API

---

## Structure du projet

```bash
pret-a-depenser/

â”‚
â”œâ”€â”€ app/                          # Code API FastAPI (dÃ©ploiement)
â”‚   â”œâ”€â”€ assets/                   # Artifacts locaux (gitignored)
â”‚   â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”‚   â””â”€â”€ model.cb
â”‚   â”‚   â””â”€â”€ api_artifacts/
â”‚   â”‚       â”œâ”€â”€ kept_features_top125_nocorr.txt
â”‚   â”‚       â”œâ”€â”€ cat_features_top125_nocorr.txt
â”‚   â”‚       â””â”€â”€ threshold_catboost_top125_nocorr.json
â”‚   â”‚
â”‚   â”œâ”€â”€ model/                    # Fonctions liÃ©es au modÃ¨le (loader/predict)
â”‚   â”‚   â”œâ”€â”€ loader.py
â”‚   â”‚   â””â”€â”€ predict.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                    # Fonctions utilitaires (validation, erreurs, IOâ€¦)
â”‚   â”‚   â”œâ”€â”€ errors.py
â”‚   â”‚   â”œâ”€â”€ io.py
â”‚   â”‚   â””â”€â”€ validation.py
â”‚   â”‚
â”‚   â”œâ”€â”€ config.py                 # Configuration (env vars / chemins / HF)
â”‚   â”œâ”€â”€ main.py                   # Point dâ€™entrÃ©e FastAPI
â”‚   â””â”€â”€ schemas.py                # SchÃ©mas Pydantic (PredictRequest, PredictResponseâ€¦)
â”‚
â”œâ”€â”€ artifacts/                    # Export notebook (artifacts API, modÃ¨le, seuil)
â”‚
â”œâ”€â”€ data/                         # DonnÃ©es (non versionnÃ©es dans GitHub)
â”‚   â”œâ”€â”€ raw/                      # DonnÃ©es brutes
â”‚   â”œâ”€â”€ clean/                    # DonnÃ©es nettoyÃ©es
â”‚   â””â”€â”€ processed/                # Dataset final prÃªt pour entraÃ®nement
â”‚
â”œâ”€â”€ examples/                     # Exemples dâ€™inputs JSON pour tester lâ€™API
â”‚   â””â”€â”€ input_example.json
â”‚
â”œâ”€â”€ htmlcov/                      # Rapport coverage pytest (auto gÃ©nÃ©rÃ©)
â”‚
â”œâ”€â”€ mlruns/                       # Dossier MLflow local (tracking runs)
â”‚
â”œâ”€â”€ notebooks/                    # Notebooks du projet (pipeline complet)
â”‚   â”œâ”€â”€ 01_data_preparation/      # PrÃ©paration + agrÃ©gation multi-tables
â”‚   â”œâ”€â”€ 02_benchmark/             # Benchmark des modÃ¨les
â”‚   â”œâ”€â”€ 03_modeling/              # Feature selection + tuning + seuil mÃ©tier
â”‚   â””â”€â”€ 04_preparation_API/       # Packaging modÃ¨le + artifacts pour API
â”‚
â”œâ”€â”€ reports/                      # Rapports, figures, exports (EDA, SHAPâ€¦)
â”‚
â”œâ”€â”€ src/                          # Code Python "mÃ©tier" utilisÃ© par les notebooks
â”‚   â”œâ”€â”€ data/                     # Fonctions de prÃ©paration des donnÃ©es
â”‚   â”œâ”€â”€ modeling/                 # EntraÃ®nement + Ã©valuation modÃ¨les
â”‚   â”œâ”€â”€ tracking/                 # MLflow logging (params, metrics, artifacts)
â”‚   â””â”€â”€ utils/                    # Helpers gÃ©nÃ©riques
â”‚
â”œâ”€â”€ tests/                        # Tests unitaires (pytest)
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ test_api.py
â”‚   â”œâ”€â”€ test_io.py
â”‚   â”œâ”€â”€ test_lifespan.py
â”‚   â”œâ”€â”€ test_loader.py
â”‚   â”œâ”€â”€ test_predict.py
â”‚   â””â”€â”€ test_validation.py
â”‚
â”œâ”€â”€ .env                          # Variables d'environnement (local uniquement)
â”œâ”€â”€ .gitignore                    # Exclusion fichiers lourds / secrets
â”œâ”€â”€ pytest.ini                    # Config pytest
â”œâ”€â”€ pyproject.toml            # DÃ©pendances Python
â””â”€â”€ README.md                     # Documentation principale
```


## Installation rapide
CrÃ©er un environnement virtuel puis installer les dÃ©pendances :

 ### API seule (prod / HF)
```bash
pip install .
```

 ### API + tests (CI)
```bash
pip install ".[dev]"
```

 ### notebooks + MLflow
```bash
pip install ".[notebooks]"
```

 ### Tout (dev + notebooks)
```bash
pip install ".[dev,notebooks]"
```

## Lancer MLflow UI (optionnel)
MLflow permet de visualiser les runs et comparer les expÃ©rimentations :
```bash
mlflow ui
```
Puis ouvrir :
http://127.0.0.1:5000

## Lancer l'API en local
```bash
uvicorn app.main:app --reload
```

API accessible sur :
http://127.0.0.1:8000
Documentation Swagger :
http://127.0.0.1:8000/docs

## Exemple d'appel API

### Healthcheck
```bash
curl http://127.0.0.1:8000/health
```
### PrÃ©diction

Un exemple de payload est disponible ici :
examples/input_example.json
Commande :

```bash
curl -X POST "http://127.0.0.1:8000/predict" \
  -H "Content-Type: application/json" \
  -d @examples/input_example.json
```

## Chargement du modÃ¨le (local ou Hugging Face)
Lâ€™API supporte deux modes de chargement des artifacts :
### Mode local
Les fichiers sont stockÃ©s dans :
app/assets/
Ce dossier est **gitignored.**
Variables :

- BUNDLE_SOURCE=local
- LOCAL_MODEL_PATH
- LOCAL_KEPT_PATH
- LOCAL_CAT_PATH
- LOCAL_THRESHOLD_PATH

### Mode Hugging Face
Les fichiers sont tÃ©lÃ©chargÃ©s depuis un repo Hugging Face.
Variables :
- BUNDLE_SOURCE=hf
- HF_REPO_ID
- HF_TOKEN (optionnel si repo public)
- HF_MODEL_PATH
- HF_KEPT_PATH
- HF_CAT_PATH
- HF_THRESHOLD_PATH

## Tests unitaires
Lancer les tests :

```bash
pytest --cov=app
```

## Run (Production / Hugging Face Space)
### Lancer en local

```bash
uvicorn app.main:app --reload --host 127.0.0.1 --port 8000
```

Swagger :
http://127.0.0.1:8000/docs

### Lancer via Docker (exemple)

```bash
docker build -t pad-api .
docker run -p 7860:7860 pad-api
```


### DÃ©ploiement automatique
Le dÃ©ploiement est automatisÃ© via GitHub Actions (CD) :
Ã  chaque push sur main, si la CI passe, le code est automatiquement synchronisÃ© et dÃ©ployÃ© sur Hugging Face Spaces.

## Secrets / SÃ©curitÃ©
### Variables dâ€™environnement
Les variables sont configurÃ©es via :
- Hugging Face Spaces â†’ Settings â†’ Variables and secrets
- GitHub â†’ Settings â†’ Secrets and variables â†’ Actions (pour le workflow CD)

Exemples de variables utilisÃ©es :
- BUNDLE_SOURCE
- HF_REPO_ID
- HF_MODEL_PATH
- HF_KEPT_PATH
- HF_CAT_PATH
- HF_THRESHOLD_PATH

### Gestion des tokens
Le token Hugging Face (HF_TOKEN) est stockÃ© uniquement en tant que Secret.
**Aucun token nâ€™est prÃ©sent dans le code source ou dans GitHub.**


### Validation des entrÃ©es API
Les inputs sont validÃ©s via :
- Pydantic (schÃ©ma dâ€™entrÃ©e)
- validate_payload() (contrÃ´le strict)
- Le systÃ¨me rejette automatiquement :
- champs manquants
- mauvais types
- champs inconnus (protection contre payload invalide / injection)


## RÃ©sultat final
Ce projet fournit :
- un modÃ¨le de scoring optimisÃ© selon un coÃ»t mÃ©tier
- un tracking complet des expÃ©rimentations via MLflow
- une API FastAPI testÃ©e (pytest)
- une architecture prÃªte pour le dÃ©ploiement Docker / Hugging Face

